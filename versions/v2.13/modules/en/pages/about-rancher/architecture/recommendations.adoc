= Architecture Recommendations
ifeval::["{build-type}" != "srfa"]
:page-languages: [en, zh]
endif::[]
:revdate: 2025-08-22
:page-revdate: {revdate}
:community-path: reference-guides/rancher-manager-architecture/architecture-recommendations 
:product-path: about-rancher/architecture/recommendations

If you are installing Rancher on a single node, the main architecture recommendation that applies to your installation is that the node running Rancher should be <<_separation_of_rancher_and_user_clusters,separate from downstream clusters.>>

== Separation of Rancher and User Clusters

A user cluster is a downstream Kubernetes cluster that runs your apps and services.

If you have a Docker installation of Rancher, the node running the Rancher server should be separate from your downstream clusters.

If Rancher is intended to manage downstream Kubernetes clusters, the Kubernetes cluster that the Rancher server runs on should also be separate from the downstream user clusters.

image::rancher-architecture-separation-of-rancher-server.svg[Separation of Rancher Server from User Clusters]

== Why HA is Better for Rancher in Production

We recommend installing the Rancher server on a high-availability Kubernetes cluster, primarily because it protects the Rancher server data. In a high-availability installation, a load balancer serves as the single point of contact for clients, distributing network traffic across multiple servers in the cluster and helping to prevent any one server from becoming a point of failure.

We don't recommend installing Rancher in a single Docker container, because if the node goes down, there is no copy of the cluster data available on other nodes and you could lose the data on your Rancher server.

=== {k3s-product-name} Kubernetes Cluster Installations

One option for the underlying Kubernetes cluster is to use K3s Kubernetes. K3s is Rancher's CNCF certified Kubernetes distribution. It is easy to install and uses half the memory of Kubernetes, all in a binary of less than 100 MB. Another advantage of K3s is that it allows an external datastore to hold the cluster data, allowing the K3s server nodes to be treated as ephemeral.

.Architecture of a K3s Kubernetes Cluster Running the Rancher Management Server
image::k3s-server-storage.svg[Architecture of a K3s Kubernetes Cluster Running the Rancher Management Server]

== Recommended Load Balancer Configuration for Kubernetes Installations

We recommend the following configurations for the load balancer and Ingress controllers:

* The DNS for Rancher should resolve to a Layer 4 load balancer (TCP).
* The Load Balancer should forward port TCP/80 and TCP/443 to all 3 nodes in the Kubernetes cluster.
* The Ingress controller will redirect HTTP to HTTPS and terminate SSL/TLS on port TCP/443.
* The Ingress controller will forward traffic to port TCP/80 on the pod in the Rancher deployment.

.Rancher installed on a Kubernetes cluster with layer 4 load balancer, depicting SSL termination at Ingress controllers
image::ha/rancher2ha.svg[Rancher HA]

== Environment for Kubernetes Installations

It is strongly recommended to install Rancher on a Kubernetes cluster on hosted infrastructure such as Amazon's EC2 or Google Compute Engine.

For the best performance and greater security, we recommend a dedicated Kubernetes cluster for the Rancher management server. Running user workloads on this cluster is not advised. After deploying Rancher, you can xref:cluster-deployment/cluster-deployment.adoc[create or import clusters] for running your workloads.

== Recommended Node Roles for Kubernetes Installations

The below recommendations apply when Rancher is installed on a K3s Kubernetes cluster.

=== {k3s-product-name} Cluster Roles

In K3s clusters, there are two types of nodes: server nodes and agent nodes. Both servers and agents can have workloads scheduled on them. Server nodes run the Kubernetes master.

For the cluster running the Rancher management server, we recommend using two server nodes. Agent nodes are not required.

== Architecture for an Authorized Cluster Endpoint (ACE)

If you are using an xref:about-rancher/architecture/communicating-with-downstream-clusters.adoc#_4_authorized_cluster_endpoint[authorized cluster endpoint (ACE),] we recommend creating an FQDN pointing to a load balancer which balances traffic across your nodes with the `controlplane` role.

If you are using private CA signed certificates on the load balancer, you have to supply the CA certificate, which will be included in the generated kubeconfig file to validate the certificate chain. See the documentation on xref:cluster-admin/manage-clusters/access-clusters/use-kubectl-and-kubeconfig.adoc[kubeconfig files] and xref:rancher-admin/users/settings/api-keys.adoc#_creating_an_api_key[API keys] for more information.

ACE support is available for registered RKE2 and K3s clusters. To view the manual steps to perform on the downstream cluster to enable the ACE, click xref:cluster-deployment/register-existing-clusters.adoc#_authorized_cluster_endpoint_support_for_rke2_and_k3s_clusters[here].
